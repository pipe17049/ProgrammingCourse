# ğŸ”¥ DÃA 2 - MIÃ‰RCOLES: "Multiprocessing + Heavy Processing"

**DuraciÃ³n**: 45min seguimiento + 1h autÃ³noma | **Objetivo**: Threading â†’ Multiprocessing para CPU-intensivos

---

## ğŸ¯ **OBJETIVOS DEL DÃA**

### **ğŸ§  CONCEPTUAL:**
- Entender diferencia entre **I/O bound** vs **CPU bound**
- Aplicar **multiprocessing** para tareas computacionalmente intensivas
- Implementar **Process Pool** escalable
- Usar **IPC Queues** para comunicaciÃ³n entre procesos

### **ğŸ› ï¸ TÃ‰CNICO:**
- Migrar filtros pesados a **ProcessPoolExecutor**
- Implementar **workers dedicados** por tipo de filtro
- **Resource monitoring** en tiempo real
- **Benchmark** threading vs multiprocessing

### **ğŸ“Š MÃ‰TRICAS DE Ã‰XITO:**
- **Speedup >3x** en filtros CPU-intensivos vs threading
- **10+ imÃ¡genes 4K** procesadas simultÃ¡neamente
- **CPU utilization >80%** durante procesamiento
- **Zero crashes** durante stress testing

---

## â° **AGENDA - 45 MINUTOS SEGUIMIENTO**

### **ğŸ”¥ WARM-UP (5 min)**
```bash
# Verificar estado actual
cd Chapter-Threads/Projects
python manage.py runserver 8000 &

# Test threading actual
curl -X POST http://localhost:8000/api/process-batch/compare/ \
     -d '{"count": 3, "filters": ["resize", "blur", "brightness"]}'
```

### **âš¡ IMPLEMENTACIÃ“N CORE (25 min)**

#### **1. Filtros Pesados Reales (8 min)**
- Implementar `heavy_sharpen_filter` con OpenCV
- Implementar `edge_detection_filter` con procesamiento real
- Test individual de filtros pesados

#### **2. Process Pool (10 min)**  
- Crear `ProcessPoolExecutor` en `processors.py`
- Migrar filtros CPU-intensivos a workers separados
- Test bÃ¡sico de multiprocessing

#### **3. Benchmark Setup (7 min)**
- Crear script de comparaciÃ³n `threading_vs_mp.py`
- MÃ©tricas: tiempo, CPU usage, memory usage
- Demo en vivo del speedup

### **ğŸ§ª DEMO FINAL (10 min)**
```bash
# ComparaciÃ³n directa - esperamos >3x speedup
python benchmarks/threading_vs_mp.py

# Stress test - 10 imÃ¡genes simultÃ¡neamente  
curl -X POST http://localhost:8000/api/process-batch/multiprocessing/ \
     -d '{"count": 10, "filters": ["sharpen", "edges"]}'
```

### **ğŸ“ WRAP-UP (5 min)**
- Review de mÃ©tricas alcanzadas
- Troubleshooting comÃºn
- Plan para 1h autÃ³noma

---

## ğŸš€ **TRABAJO AUTÃ“NOMO - 1 HORA**

### **ğŸ¯ OBJETIVOS AUTÃ“NOMOS:**
1. **Workers especializados** por tipo de operaciÃ³n
2. **Queue-based IPC** para comunicaciÃ³n robusta  
3. **Resource monitoring** con alertas
4. **Error handling** y recovery

### **ğŸ“‹ CHECKLIST AUTÃ“NOMO:**

#### **ğŸ”§ WORKERS AVANZADOS (25 min)**
- [ ] `FilterWorker` class con especializaciÃ³n por filtro
- [ ] Worker pools dedicados (`io_pool`, `cpu_pool`) 
- [ ] Worker lifecycle management (start/stop/restart)
- [ ] Load balancing entre workers disponibles

#### **ğŸ”— IPC COMMUNICATION (20 min)**
- [ ] `QueueManager` con multiprocessing.Queue
- [ ] Message passing entre API y workers
- [ ] Request/response correlation
- [ ] Queue monitoring y stats

#### **ğŸ“Š MONITORING (10 min)**
- [ ] Real-time CPU/memory tracking con `psutil`
- [ ] Worker health checks
- [ ] Performance alerts (>90% CPU)
- [ ] Metrics dashboard simple

#### **ğŸ›¡ï¸ ERROR HANDLING (5 min)**
- [ ] Worker crash detection y restart
- [ ] Timeout handling para filtros pesados
- [ ] Graceful degradation (fallback a threading)

---

## ğŸ“‚ **ARCHIVOS A CREAR HOY**

### **ğŸ†• NUEVOS ARCHIVOS:**
```
Projects/
â”œâ”€â”€ workers/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ filter_worker.py     # â† ProcessPoolExecutor workers
â”‚   â”œâ”€â”€ queue_manager.py     # â† IPC con multiprocessing.Queue  
â”‚   â””â”€â”€ monitor.py           # â† psutil monitoring
â”œâ”€â”€ benchmarks/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ threading_vs_mp.py   # â† Performance comparison
â””â”€â”€ tests/
    â””â”€â”€ test_multiprocessing.py # â† Unit tests MP
```

### **ğŸ“ ARCHIVOS A ACTUALIZAR:**
- `image_api/filters.py` â†’ Implementar filtros pesados reales
- `image_api/processors.py` â†’ Agregar multiprocessing support
- `image_api/views.py` â†’ Endpoints multiprocessing
- `requirements_extended.txt` â†’ Agregar OpenCV, psutil

---

## ğŸ¨ **FILTROS DEL DÃA**

### **ğŸ§µ THREADING (DÃ­a 1):**
- `resize` â†’ 0.2s (I/O + resize simple)
- `blur` â†’ 0.3s (I/O + blur ligero)  
- `brightness` â†’ 0.1s (I/O + ajuste simple)

### **ğŸ”„ MULTIPROCESSING (DÃ­a 2):**
- `heavy_sharpen` â†’ 2.0s (CPU intensivo)
- `edge_detection` â†’ 1.5s (CPU + algoritmos complejos)

### **ğŸ“Š PERFORMANCE ESPERADO:**
```
THREADING (3 filtros ligeros):
â”œâ”€â”€ Sequential: ~0.6s Ã— 3 imÃ¡genes = 1.8s
â””â”€â”€ Parallel: ~0.6s (speedup 3x)

MULTIPROCESSING (2 filtros pesados):  
â”œâ”€â”€ Sequential: ~3.5s Ã— 3 imÃ¡genes = 10.5s
â”œâ”€â”€ Threading: ~3.5s (GIL limitation)
â””â”€â”€ Multiprocessing: ~3.5s / cores (speedup 4x+ en quad-core)
```

---

## ğŸ§ª **COMANDOS DE TESTING**

### **âš¡ QUICK TESTS:**
```bash
# 1. Test filtros pesados individuales
curl -X POST http://localhost:8000/api/filters/sharpen/ \
     -F "image=@static/images/sample_4k.jpg"

# 2. Benchmark automÃ¡tico
python benchmarks/threading_vs_mp.py --images=5 --verbose

# 3. Resource monitoring
python workers/monitor.py --duration=30
```

### **ğŸ”¥ STRESS TESTS:**
```bash
# Procesar 10 imÃ¡genes 4K simultÃ¡neamente
curl -X POST http://localhost:8000/api/process-batch/stress/ \
     -d '{"count": 10, "filters": ["sharpen", "edges", "resize"]}'

# Monitor durante stress test
python workers/monitor.py --stress-mode
```

---

## ğŸ“ **CONCEPTOS CLAVE PARA ESTUDIANTES**

### **ğŸ§  I/O BOUND vs CPU BOUND:**
```python
# I/O BOUND (threading wins)
def resize_filter(image):
    image_data = open(file).read()  # â† Disk I/O
    return simple_resize(image_data)

# CPU BOUND (multiprocessing wins)  
def edge_detection(image):
    for pixel in millions_of_pixels:    # â† Pure computation
        complex_algorithm(pixel)
```

### **âš¡ GIL IMPACT:**
- **Threading**: Perfect para I/O, limitado por GIL en CPU
- **Multiprocessing**: Bypass GIL, cada proceso = core dedicado

### **ğŸ”— IPC PATTERNS:**
- **Queues**: Para task distribution
- **Pipes**: Para worker communication  
- **Shared Memory**: Para data sharing

---

## ğŸš¨ **TROUBLESHOOTING COMÃšN**

### **âŒ PROBLEMA**: "ProcessPoolExecutor hangs"
**âœ… SOLUCIÃ“N**: 
```python
# Timeout para evitar hangs
with ProcessPoolExecutor() as executor:
    future = executor.submit(heavy_filter, image)
    result = future.result(timeout=30)
```

### **âŒ PROBLEMA**: "Queue memory overflow"
**âœ… SOLUCIÃ“N**:
```python
# Limitar queue size
queue = multiprocessing.Queue(maxsize=100)
```

### **âŒ PROBLEMA**: "Workers not releasing memory"
**âœ… SOLUCIÃ“N**: Restart workers periodically

---

## ğŸ“Š **CRITERIOS DE EVALUACIÃ“N**

### **ğŸŸ¢ BÃSICO (todos deben lograr):**
- [ ] 2+ filtros pesados funcionando
- [ ] ProcessPoolExecutor con speedup >2x
- [ ] Benchmark script ejecutable

### **ğŸŸ¡ INTERMEDIO (mayorÃ­a deberÃ­a lograr):**
- [ ] Workers especializados por filtro
- [ ] Queue-based communication
- [ ] Resource monitoring bÃ¡sico
- [ ] Error handling robusto

### **ğŸ”´ AVANZADO (algunos lograrÃ¡n):**
- [ ] Auto-scaling de worker pools
- [ ] Real-time metrics dashboard  
- [ ] Intelligent load balancing
- [ ] Graceful degradation

---

## ğŸ¯ **PREPARACIÃ“N PARA MAÃ‘ANA (DÃA 3)**

**Objetivo DÃ­a 3**: Workers locales â†’ **Workers distribuidos** en Docker containers

**Setup requerido**:
- Redis server para distributed queues
- Docker containers preparados
- Load balancer bÃ¡sico

Â¡**Hoy construimos la fundaciÃ³n multiprocessing** para maÃ±ana escalar distribuidamente! ğŸš€ 