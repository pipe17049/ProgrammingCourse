# ğŸ–¼ï¸ Pipeline de Procesamiento de ImÃ¡genes Distribuido

**Proyecto de 4 dÃ­as: De Threading a Sistemas Distribuidos con Monitoreo Real** âœ…

Este proyecto evoluciona desde un servidor Django bÃ¡sico hasta un **sistema distribuido de procesamiento de imÃ¡genes** completo con **monitoreo en tiempo real**, demostrando conceptos de concurrencia, paralelismo, arquitecturas distribuidas y mÃ©tricas de producciÃ³n.

## ğŸ¯ Objetivos del Proyecto

### **ğŸ“… DÃA 1: Foundation** 
- âœ… **I/O-bound operations**: Leer archivos grandes del disco
- âœ… **Threading vs Multiprocessing**: ComparaciÃ³n de rendimiento  
- âœ… **Load testing**: MediciÃ³n de concurrencia

### **ğŸ“… DÃA 2: Real Processing**
- âœ… **Filtros reales**: PIL (Pillow) y OpenCV 
- âœ… **CPU-bound tasks**: Blur, sharpen, edge detection, resize
- âœ… **Performance benchmarking**: Sequential vs Threading vs Multiprocessing
- âœ… **Resource monitoring**: CPU, memory, processing time

### **ğŸ“… DÃA 3: Distributed System** 
- âœ… **Sistema distribuido**: Redis + Worker containers
- âœ… **Task distribution**: FIFO queue distribuyendo tareas entre workers especializados
- âœ… **Fault tolerance**: Worker registration, heartbeat, failure handling
- âœ… **Docker orchestration**: docker-compose con mÃºltiples servicios
- âœ… **Monitoring**: Worker status, task tracking, performance metrics

### **ğŸ“… DÃA 4: Smart Monitoring & Metrics** âœ… **COMPLETADO**
- âœ… **Sistema de mÃ©tricas real**: CPU, memoria, utilizaciÃ³n de workers en tiempo real
- âœ… **DetecciÃ³n de carga**: Queue length, busy workers, success rate
- âœ… **Recomendaciones educativas**: CuÃ¡ndo escalar workers (sin ejecuciÃ³n automÃ¡tica)
- âœ… **Dashboard tiempo real**: Terminal UI mostrando mÃ©tricas en vivo
- âœ… **Stress testing funcional**: Scripts para generar carga y ver mÃ©tricas cambiar
- âœ… **Debugging completo**: Resueltos timeouts, mÃ©tricas incorrectas, Docker issues

### **ğŸ“… DÃA 5: Real Auto-Scaling con Kubernetes** âœ… **COMPLETADO**
- âœ… **MigraciÃ³n a Kubernetes**: De docker-compose a K8s deployments
- âœ… **ImÃ¡genes optimizadas**: ReducciÃ³n de 6GB â†’ 2.2GB (63% optimizaciÃ³n)
- âœ… **Horizontal Pod Autoscaler (HPA)**: Auto-scaling real basado en CPU/memoria
- âœ… **Metrics Server**: ConfiguraciÃ³n especÃ­fica para Docker Desktop
- âœ… **Cross-platform**: Compatible con Windows, macOS (ARM64/Intel) y Linux
- âœ… **Demo funcional**: Auto-scaling funcionando con mÃ©tricas reales
- âœ… **Troubleshooting completo**: SoluciÃ³n de errores comunes de K8s en desarrollo

## ğŸ—ï¸ Arquitectura del Sistema

```
                    ğŸŒ Client
                (curl requests)
                       |
                   ğŸ Django API
                    :8000
                 (Single Instance)
                       |
                ğŸ“¡ Redis Queue
              (Task Distribution
               Worker Registry)
                   /  |  \
                  /   |   \
                 /    |    \
            ğŸ‘· Worker-1  ğŸ‘· Worker-2  ğŸ‘· Worker-3
           I/O Specialist CPU Specialist General Purpose
           resize, blur,   sharpen,      ALL FILTERS
           brightness     edges            |
                |           |              |
                |           |              |
           ğŸ–¼ï¸ Static Images â†â†’ ğŸ’¾ Processed Images
           sample_4k.jpg      static/processed/
           misurina-sunset.jpg
                
    ğŸ“Š Smart Monitoring System (Day 4) âœ…
    â”œâ”€â”€ ğŸ”¥ CPU Usage & ğŸ§  Memory Usage (psutil)
    â”œâ”€â”€ âš¡ Busy Workers & ğŸ“ˆ Worker Utilization  
    â”œâ”€â”€ ğŸ“‹ Queue Length & âœ… Success Rate
    â”œâ”€â”€ ğŸ¬ Scaling Recommendations (Educational)
    â”œâ”€â”€ ğŸ“Š Real-time Terminal Dashboard
    â””â”€â”€ ğŸš€ Stress Testing Scripts (5 tipos)
```

### **ğŸ”„ Flujo de Procesamiento:**

```
1. ğŸ“¤ Client: POST /api/process-batch/distributed/
                    â†“
2. ğŸ Django API: Crea task_id Ãºnico y encola en Redis (LPUSH)
                    â†“
3. ğŸ“¡ Redis Queue: [task1, task2, task3] â†’ Workers pull (BRPOP)
                    â†“
4. ğŸ‘· Worker: Toma prÃ³ximo task disponible (FIFO)
                    â†“
5. ğŸ” Worker: Revisa capabilities DESPUÃ‰S de tomar task
                    â†“
6a. âœ… Compatible: Procesa filtros â†’ Guarda en static/processed/
6b. âŒ Incompatible: Marca task como FAILED (ğŸ’€ Tarea perdida)
                    â†“
7. ğŸ“Š Client: Consulta status con /api/task/{task_id}/status/
```

## ğŸš€ Setup y EjecuciÃ³n

### **OpciÃ³n A: Setup Local (DÃ­as 1-2)**

```bash
# 1. Instalar dependencias
pip install -r requirements.txt

# 2. Crear directorios necesarios
mkdir -p static/processed

# 3. Ejecutar servidor local
python manage.py runserver 8000
```

### **OpciÃ³n B: Setup Distribuido con Docker (DÃ­a 3)**

```bash
# 1. Construir imÃ¡genes
docker-compose build

# 2. Levantar sistema completo
docker-compose up -d

# 3. Verificar servicios
docker-compose ps
```

### **Verificar instalaciÃ³n:**
```bash
# Health check
curl http://localhost:8000/api/health/

# Ver workers activos (solo Docker)
curl http://localhost:8000/api/workers/status/
```

## âš¡ **DEMO RÃPIDO: Ver MÃ©tricas Cambiar** 

**ğŸ¯ Para ver el sistema funcionando inmediatamente:**

```bash
# Terminal 1: Ver mÃ©tricas limpias
python simple_monitoring/cli.py metrics
# âš¡ Busy Workers: 0    ğŸ“ˆ Utilization: 0.0%

# Terminal 2: Lanzar stress test  
cd k8s && python stress_test.py 3 15

# Terminal 1: Ver mÃ©tricas cambiar INMEDIATAMENTE
python simple_monitoring/cli.py metrics  
# âš¡ Busy Workers: 3    ğŸ“ˆ Utilization: 100.0%
# ğŸ¬ Action: MAINTAIN   ğŸ“ Reason: System at optimal capacity
```

**ğŸ”¥ Resultado**: En **< 5 segundos** verÃ¡s workers pasar de 0% a 100% utilizaciÃ³n con recomendaciones inteligentes.

## ğŸ§ª Testing y Comandos

### **ğŸ“… DÃA 1: Endpoints BÃ¡sicos**

```bash
# Health check
curl http://localhost:8000/api/health/

# InformaciÃ³n de imagen (rÃ¡pido)
curl http://localhost:8000/api/image/info/

# Descargar imagen 4K (I/O-bound)  
curl http://localhost:8000/api/image/4k/ -o downloaded_4k.jpg

# Imagen con procesamiento lento
curl "http://localhost:8000/api/image/slow/?delay=3.0" -o slow_4k.jpg

# EstadÃ­sticas del servidor
curl http://localhost:8000/api/stats/
```

### **ğŸ“… DÃA 2: Filtros Reales (PIL/OpenCV)**

```bash
# Procesamiento con filtros secuencial
curl -X POST http://localhost:8000/api/process-batch/sequential/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["resize", "blur"], "filter_params": {"resize": {"width": 800, "height": 600}, "blur": {"radius": 3.0}}}'

# Procesamiento con threading
curl -X POST http://localhost:8000/api/process-batch/threading/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["sharpen", "edges"]}'

# Procesamiento con multiprocessing
curl -X POST http://localhost:8000/api/process-batch/multiprocessing/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["brightness"], "filter_params": {"brightness": {"factor": 1.5}}}'

# Comparar todos los mÃ©todos
curl -X POST http://localhost:8000/api/process-batch/compare-all/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["resize", "blur"]}'

# Stress test
curl -X POST http://localhost:8000/api/process-batch/stress/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["sharpen", "edges"], "num_iterations": 5}'
```

### **ğŸ“… DÃA 3: Sistema Distribuido (Docker)**

```bash
# Procesamiento distribuido
curl -X POST http://localhost:8000/api/process-batch/distributed/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["resize", "sharpen", "edges"], "filter_params": {"resize": {"width": 1024, "height": 768}}}'

# Estado de workers
curl http://localhost:8000/api/workers/status/ | python -m json.tool

# Monitoreo en tiempo real
watch -n 2 'curl -s http://localhost:8000/api/workers/status/ | python -m json.tool'

# Consultar estado de task individual (usar task_id de la respuesta anterior)
curl http://localhost:8000/api/task/{TASK_ID}/status/ | python -m json.tool
```

### **ğŸ¯ Testing Worker Specialization**

```bash
# Test: Solo worker-2 puede hacer 'sharpen'
# 1. Parar worker-3: docker-compose stop worker-3
# 2. Enviar mÃºltiples tareas sharpen:
for i in {1..5}; do
  curl -X POST http://localhost:8000/api/process-batch/distributed/ \
    -H "Content-Type: application/json" \
    -d '{"filters": ["sharpen"]}' &
done

# 3. Ver resultados: worker-1 falla, worker-2 procesa
curl http://localhost:8000/api/workers/status/
```

### **ğŸ” Testing Job Failure vs Worker Failure**

```bash
# SCENARIO 1: Job Failure (Worker incompatible)
# 1. Enviar task que worker-1 no puede manejar
curl -X POST http://localhost:8000/api/process-batch/distributed/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["sharpen"]}' \
  | jq '.task_id' # Guardar task_id

# 2. Consultar status especÃ­fico del job
curl http://localhost:8000/api/task/{TASK_ID}/status/ | jq '
{
  status: .status,
  failure_type: .failure_type,
  failure_reason: .failure_reason,
  explanation: .explanation,
  error: .error
}'

# RESPUESTA de Job Failure:
# {
#   "status": "failed",
#   "failure_type": "job_failure",
#   "failure_reason": "worker_capability_mismatch", 
#   "explanation": "Worker tomÃ³ task pero no puede manejar el filtro requerido",
#   "error": "Worker worker-1 cannot handle filters: ['sharpen']"
# }

# SCENARIO 2: Worker Failure (Worker caÃ­do)
# 1. Parar todos los workers
docker-compose stop worker-1 worker-2 worker-3

# 2. Enviar task
curl -X POST http://localhost:8000/api/process-batch/distributed/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["resize"]}' \
  | jq '.task_id'

# 3. Consultar despuÃ©s de timeout
curl http://localhost:8000/api/task/{TASK_ID}/status/ | jq '
{
  status: .status,
  explanation: "No workers available to process task"
}'

# RESPUESTA de Worker Failure:
# {
#   "status": "pending",  # Task nunca fue tomada
#   "explanation": "No workers available to process task"
# }
```

## ğŸ“Š Endpoints Disponibles

### **DÃA 1: BÃ¡sicos**
| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/api/health/` | GET | Health check del servidor |
| `/api/image/info/` | GET | Metadata de imagen sin transferir |
| `/api/image/4k/` | GET | Descargar imagen 4K (I/O-bound) |
| `/api/image/slow/?delay=N` | GET | Imagen con delay simulado |
| `/api/stats/` | GET | EstadÃ­sticas del servidor |

### **DÃA 2: Procesamiento Real**
| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/api/process-batch/sequential/` | POST | Procesamiento secuencial con filtros |
| `/api/process-batch/threading/` | POST | Procesamiento con threading |
| `/api/process-batch/multiprocessing/` | POST | Procesamiento con multiprocessing |
| `/api/process-batch/compare/` | POST | Comparar threading vs multiprocessing |
| `/api/process-batch/compare-all/` | POST | Comparar todos los mÃ©todos |
| `/api/process-batch/stress/` | POST | Test de estrÃ©s con mÃºltiples iteraciones |

### **DÃA 3: Sistema Distribuido**
| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/api/process-batch/distributed/` | POST | Procesamiento distribuido con workers |
| `/api/workers/status/` | GET | Estado de todos los workers |
| `/api/task/<task_id>/status/` | GET | **Estado de task individual** (job failure vs worker failure) |

### **DÃA 4: Sistema de Monitoreo** âœ…
| Endpoint | MÃ©todo | DescripciÃ³n |
|----------|--------|-------------|
| `/api/metrics/` | GET | **MÃ©tricas del sistema** (CPU, memoria, workers, recomendaciones) |

### **Comandos CLI de Monitoreo:**
| Comando | DescripciÃ³n |
|---------|-------------|
| `python simple_monitoring/cli.py check` | Verificar API disponible |
| `python simple_monitoring/cli.py metrics` | Ver mÃ©tricas actuales |
| `python simple_monitoring/cli.py monitor` | Dashboard en tiempo real |
| `python simple_monitoring/cli.py stress 10` | Stress test via API |

### **Comandos de Limpieza:**
| Comando | DescripciÃ³n |
|---------|-------------|
| **Limpiar imÃ¡genes procesadas:** | `rm -rf static/processed/*` (Linux/Mac) |
| | `Remove-Item static\processed\* -Force` (PowerShell) |
| | `del /q static\processed\*` (CMD Windows) |
| **Purgar Redis (Docker):** | `docker exec image_processing_redis redis-cli FLUSHALL` |
| **Purgar Redis (K8s):** | `kubectl exec deployment/redis-deployment -- redis-cli FLUSHALL` |

### **Filtros Disponibles:**
- **`resize`**: Cambiar tamaÃ±o (PIL) - I/O-bound
- **`blur`**: Difuminado gaussiano (PIL) - I/O-bound  
- **`brightness`**: Ajuste de brillo (PIL) - I/O-bound
- **`sharpen`**: Nitidez avanzada (OpenCV) - CPU-bound
- **`edges`**: DetecciÃ³n de bordes (OpenCV) - CPU-bound

## ğŸ” AnÃ¡lisis de Rendimiento

### **ğŸƒâ€â™‚ï¸ DÃA 2: Threading vs Multiprocessing**

**Para filtros I/O-bound (resize, blur, brightness):**
- âœ… **Threading wins**: ~3-5x mÃ¡s rÃ¡pido que secuencial
- âš¡ **Multiprocessing**: ~2-3x mÃ¡s rÃ¡pido (overhead de procesos)
- ğŸ§  **RazÃ³n**: GIL se libera durante I/O, threading es mÃ¡s eficiente

**Para filtros CPU-bound (sharpen, edges):**
- âœ… **Multiprocessing wins**: ~4-6x mÃ¡s rÃ¡pido que secuencial  
- ğŸŒ **Threading**: ~1.2x mÃ¡s rÃ¡pido (limitado por GIL)
- ğŸ§  **RazÃ³n**: CPU-bound necesita verdadero paralelismo

### **ğŸŒ DÃA 3: Sistema Distribuido**

**CaracterÃ­sticas del FIFO Queue:**
- âš–ï¸ **Load Balancing**: Simple FIFO, no inteligente
- âŒ **Fault Tolerance**: Tareas fallan si worker incompatible las toma
- ğŸ¯ **Worker Specialization**: Configurado por `WORKER_CAPABILITIES`
- ğŸ“Š **Monitoring**: Worker registry con heartbeat

**Worker-3 como "Salvador":**
- ğŸ›¡ï¸ Worker-3 (`capabilities=all`) previene fallos
- ğŸ² DistribuciÃ³n basada en timing, no capabilities
- âš ï¸ Si worker-3 se cae, tareas incompatibles fallan para siempre

## ğŸ“Š **DÃA 4: Sistema de Monitoreo Real** âœ…

### **ğŸ¯ MÃ©tricas en Tiempo Real**

El sistema incluye **monitoreo inteligente** que recopila mÃ©tricas reales del sistema:

```bash
# Ver mÃ©tricas actuales
python simple_monitoring/cli.py metrics

# Resultado example:
ğŸ“Š SYSTEM METRICS
========================================
ğŸ”¥ CPU Usage:          30.5%
ğŸ§  Memory Usage:       69.5%
ğŸ’½ Memory Available:    2.3 GB

âš™ï¸ WORKER METRICS
--------------------
ğŸ‘¥ Active Workers:        3
âš¡ Busy Workers:          3
ğŸ“ˆ Utilization:      100.0%
ğŸ“‹ Queue Length:          0
âœ… Success Rate:      100.0%

ğŸ“ SCALING RECOMMENDATION (Educational)
----------------------------------------
ğŸ“Š Current Workers:       3
ğŸ¯ Recommended:           3
ğŸ¬ Action:           MAINTAIN
ğŸ“ Reason:           System operating within optimal parameters
ğŸ¯ Confidence:        80.0%
âš¡ Urgency:          NONE
```

### **ğŸš€ Stress Testing Scripts**

**Script unificado de stress test** multiplataforma para generar carga:

```bash
cd k8s
# Uso: python stress_test.py [minutos] [tareas_por_batch]

# 1. BURST STRESS - Carga rÃ¡pida (Kubernetes auto-scaling)
python stress_test.py 5 20       # 5 minutos, 20 tareas por batch

# 2. CONTINUOUS STRESS - Carga sostenida  
python stress_test.py 10 15      # 10 minutos, 15 tareas por batch
```

### **ğŸ“ˆ Ver MÃ©tricas Cambiar en Tiempo Real**

```bash
# Terminal 1: Lanzar stress test
cd k8s && python stress_test.py 5 15

# Terminal 2: Ver mÃ©tricas cambiar inmediatamente  
python simple_monitoring/cli.py metrics

# Antes del stress:
âš¡ Busy Workers: 0    ğŸ“ˆ Utilization: 0.0%

# Durante el stress:
âš¡ Busy Workers: 3    ğŸ“ˆ Utilization: 100.0%
ğŸ¬ Action: MAINTAIN   ğŸ“ Reason: System at optimal capacity
```

### **ğŸ›ï¸ Dashboard en Tiempo Real**

```bash
# Terminal interactivo con mÃ©tricas en vivo
python simple_monitoring/cli.py monitor

# Actualiza mÃ©tricas cada 2 segundos mostrando:
# - CPU/Memory usage en tiempo real
# - Worker utilization dinÃ¡mica  
# - Recomendaciones que cambian con la carga
# - Success rate y estadÃ­sticas
```

## ğŸ› ï¸ Troubleshooting

### **Local Setup Issues**
```bash
# Dependencias faltantes
pip install -r requirements.txt

# Directorios faltantes  
mkdir -p static/processed static/images

# Puerto en uso
python manage.py runserver 8080
```

### **Docker Issues**
```bash
# Servicios no inician
docker-compose down && docker-compose up --build

# Redis connection failed
docker-compose logs redis

# Workers no registran
docker-compose logs worker-1
```

### **Workers No Processan**
```bash
# Verificar workers activos
curl http://localhost:8000/api/workers/status/

# Verificar cola Redis
docker-compose exec redis redis-cli LLEN task_queue

# Restart workers
docker-compose restart worker-1 worker-2 worker-3
```

### **ğŸ“Š Problemas de Monitoreo (Resueltos)** âœ…

```bash
# PROBLEMA: MÃ©tricas muestran "Busy Workers: 0" durante alta carga
# CAUSA: API container usando cÃ³digo viejo
# SOLUCIÃ“N: Rebuild agresivo del container
docker-compose stop api && docker-compose rm -f api
docker rmi projects-api
docker-compose build api --no-cache && docker-compose up -d api

# PROBLEMA: API timeouts durante stress tests  
# CAUSA: API esperando sincrÃ³nicamente tareas distribuidas
# SOLUCIÃ“N: Endpoint distribuido retorna task_id inmediatamente

# PROBLEMA: MÃ©tricas incorrectas en Redis
# CAUSA: Datos viejos acumulados de tests anteriores
# SOLUCIÃ“N: Purgar Redis entre tests
docker exec image_processing_redis redis-cli FLUSHALL

# VERIFICAR: MÃ©tricas funcionando correctamente
python simple_monitoring/cli.py metrics
# Debe mostrar busy workers > 0 durante carga alta
```

## ğŸ† Logros del Proyecto

### **ğŸ“ˆ ProgresiÃ³n TÃ©cnica:**
1. **DÃ­a 1**: Servidor bÃ¡sico I/O-bound â†’ Threading fundamentals
2. **DÃ­a 2**: Filtros reales PIL/OpenCV â†’ CPU vs I/O bound analysis  
3. **DÃ­a 3**: Sistema distribuido â†’ Redis, Docker, Load balancing
4. **DÃ­a 4**: **Sistema de monitoreo completo** â†’ MÃ©tricas reales, stress testing, debugging

### **ğŸ¯ Conceptos Demostrados:**
- âœ… **GIL Impact**: Threading vs Multiprocessing en diferentes workloads
- âœ… **Real-world Libraries**: PIL, OpenCV, Redis en production
- âœ… **Distributed Architectures**: Message queues, worker pools, fault tolerance
- âœ… **DevOps Integration**: Docker, docker-compose, multi-service systems
- âœ… **Performance Analysis**: Benchmarking, monitoring, bottleneck identification
- âœ… **Real-time Monitoring**: CPU/Memory tracking, worker utilization metrics
- âœ… **Stress Testing**: 5 tipos de scripts para generar carga controlada
- âœ… **Production Debugging**: Resolver timeouts, mÃ©tricas incorrectas, Docker issues

### **ğŸš€ Logros Ãšnicos de este Proyecto:**
- ğŸ¯ **MÃ©tricas que cambian en tiempo real** - Ver utilizaciÃ³n de workers subir de 0% a 100%
- ğŸ”„ **Debugging sistemÃ¡tico** - Resolver problemas reales de desarrollo distribuido
- ğŸ“Š **Monitoreo educativo** - Recomendaciones de scaling sin ejecuciÃ³n automÃ¡tica  
- âš¡ **Stress testing cientÃ­fico** - Scripts controlados para generar cargas especÃ­ficas
- ğŸ³ **Docker debugging avanzado** - Resolver containers usando cÃ³digo viejo
- ğŸ“ˆ **Performance real** - 300+ tareas procesadas, workers al 100% utilizaciÃ³n

---

**ğŸš€ De conceptos bÃ¡sicos de concurrencia a sistemas distribuidos con monitoreo real en 4 dÃ­as!**

## ğŸ–¥ï¸ **SETUP PARA WINDOWS**

### **InstalaciÃ³n rÃ¡pida:**
```cmd
# 1. Ir a Projects folder
cd ProgrammingCourse\Chapter-Threads\Projects

# 2. Crear entorno virtual
python -m venv venv
venv\Scripts\activate

# 3. Instalar dependencias
pip install -r requirements.txt

# 4. Crear directorio
mkdir static\processed

# 5. Verificar
python manage.py check

# 6. Iniciar servidor
python manage.py runserver 8000
```

### **Endpoints disponibles por dÃ­a:**

**ğŸ“… DÃ­a 1-2 (Threading/Multiprocessing):**
- `/api/process-batch/sequential/` - Procesamiento secuencial
- `/api/process-batch/threading/` - Con threading  
- `/api/process-batch/multiprocessing/` - Con multiprocessing
- `/api/process-batch/compare-all/` - Comparar todos los mÃ©todos

**ğŸ“… DÃ­a 3-4 (Distributed + Monitoring):**
- `/api/process-batch/distributed/` - Sistema distribuido con Redis + Workers
- `/api/metrics/` - MÃ©tricas del sistema en tiempo real

**ğŸ“… DÃ­a 5 (Kubernetes):**
- **Mismo endpoint:** `/api/process-batch/distributed/` 
- **Diferencia:** Ahora corre en **pods auto-escalables** ğŸš€

### **Ejemplo de uso en K8s:**
```bash
# 1. Port-forward EN OTRA TERMINAL (se queda corriendo):
kubectl port-forward service/api-service 8000:8000

# 2. Ahora puedes usar el mismo endpoint desde tu PC:
curl -X POST http://localhost:8000/api/process-batch/distributed/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["resize", "blur"]}'
```

**ğŸ’¡ TIP:** El port-forward es como un "cable virtual" que conecta tu PC con Kubernetes.

---

## ğŸš€ **KUBERNETES: Auto-Scaling Real (DÃ­a 5)**

### **ğŸ¯ Â¿Por quÃ© Kubernetes?**

Docker Compose es excelente para desarrollo, pero tiene limitaciones:
- âŒ **No hay auto-scaling real** - Los nombres de containers son fijos
- âŒ **Scaling manual** - `docker-compose scale worker=5` no es automÃ¡tico
- âŒ **Sin mÃ©tricas integradas** - No puede escalar basado en CPU/memoria

**Kubernetes soluciona esto con:**
- âœ… **Auto-scaling automÃ¡tico** - HPA (Horizontal Pod Autoscaler)
- âœ… **MÃ©tricas integradas** - CPU, memoria, mÃ©tricas custom
- âœ… **Escalado inteligente** - Basado en carga real
- âœ… **Tolerancia a fallos** - Pods se recrean automÃ¡ticamente

### **ğŸ“Š Arquitectura Kubernetes**

```
                    ğŸŒ Client
                (kubectl port-forward)
                       |
                   ğŸ API Service
                  (LoadBalancer)
                       |
                ğŸ“¡ Redis Service 
               (ClusterIP)
                   /  |  \
                  /   |   \
             ğŸ‘· Worker Pod  ğŸ‘· Worker Pod  
            (CPU: 100m-200m) (Memory: 128Mi-256Mi)
                  |               |
                  â””â”€â”€â”€ HPA â”€â”€â”€â”€â”€â”€â”€â”˜
               (CPU target: 70%)
              (Memory target: 80%)
                      |
               ğŸ“Š Metrics Server
              (Recolecta mÃ©tricas)
```

### **ğŸ”§ Quick Start: Demo Kubernetes**

#### **1. Prerequisitos:**
```bash
# Verificar que Kubernetes estÃ© habilitado en Docker Desktop
kubectl version --client
kubectl cluster-info
```

#### **2. Setup automÃ¡tico (recomendado):**
```bash
# Setup completo (Docker + Kubernetes)
python setup.py

# O setup por partes:
python setup.py --docker-only    # Solo Docker Compose
python setup.py --k8s-only       # Solo Kubernetes
python setup.py --check          # Verificar prerequisitos
```

#### **2b. Build manual (alternativo):**
```bash
# Construir imÃ¡genes optimizadas (2.2GB vs 6GB originales)
python build.py

# Si los cambios no se aplican (problema comÃºn en Windows):
python build.py --clean
# O build sin cachÃ©:
docker build --no-cache -f docker/Dockerfile.api.final -t projects-api-final:latest .

# Verificar imÃ¡genes
docker images | grep projects-.*-final
```

#### **3. Ejecutar demo completo (multiplataforma):**
```bash
cd k8s
python demo.py
```

**ğŸŒŸ NUEVO: Demo 100% multiplataforma:**
- âœ… **Windows**: Detecta PowerShell automÃ¡ticamente, usa `find /c` en lugar de `wc -l`
- âœ… **Linux/Mac**: Usa comandos nativos `curl` y `grep`
- âœ… **Auto-detecciÃ³n**: Detecta si `requests` estÃ¡ disponible para stress test avanzado
- âœ… **Fallback inteligente**: Si falta `requests`, usa `curl` multiplataforma

El demo automÃ¡ticamente:
- âœ… **Despliega Redis, API y Workers**
- âœ… **Configura HPA optimizado** (escalado rÃ¡pido + descalado en 1min)
- âœ… **Instala Metrics Server** (especÃ­fico para Docker Desktop)
- âœ… **Stress test real** con procesamiento de imÃ¡genes
- âœ… **Muestra escalado Y descalado** en tiempo real
- âœ… **Verifica que todo funcione**
- âœ… **Muestra mÃ©tricas reales**: `cpu: 1%/70%, memory: 27%/80%`

### **ğŸ“Š Â¿QuÃ© es el Metrics Server y por quÃ© es necesario?**

#### **ğŸ¤” Problema sin Metrics Server:**
```bash
kubectl get hpa
NAME         TARGETS
worker-hpa   <unknown>/70%  âŒ HPA "ciego" - no puede medir
```

#### **âœ… SoluciÃ³n con Metrics Server:**
```bash
kubectl get hpa  
NAME         TARGETS
worker-hpa   cpu: 1%/70%, memory: 27%/80%  âœ… HPA "inteligente" 
```

#### **ğŸ” Â¿Por quÃ© hay que instalarlo?**

**En clusters reales (producciÃ³n):**
- **AWS EKS, Google GKE, Azure AKS:** âœ… Viene preinstalado

**En desarrollo local:**
- **Docker Desktop, minikube, kind:** âŒ Hay que instalarlo manualmente

#### **ğŸ¯ Â¿CÃ³mo funciona?**
```mermaid
graph TD
    A[Pods ejecutÃ¡ndose] --> B[Metrics Server]
    B --> C[Recolecta CPU/Memory cada 15s]
    C --> D[HPA Controller]
    D --> E{CPU > 70%?}
    E -->|SÃ­| F[kubectl scale deployment worker +2 pods]
    E -->|No| G[Mantener pods actuales]
    F --> A
    G --> A
```

**AnalogÃ­a:** Es como un **termostato con termÃ³metro**
- **Sin metrics server:** Termostato sin termÃ³metro (no sabe la temperatura)
- **Con metrics server:** Puede medir y tomar decisiones inteligentes

### **âš¡ ConfiguraciÃ³n de Descalado Optimizada**

**ğŸš¨ Problema comÃºn:** El descalado por defecto es MUY lento (5 minutos)
```yaml
# âŒ ANTES: ConfiguraciÃ³n por defecto
# stabilizationWindowSeconds: 300  # 5 minutos!
```

**âœ… SOLUCIÃ“N: HPA optimizado para demos:**
```yaml
behavior:
  scaleUp:
    stabilizationWindowSeconds: 30  # Escalado rÃ¡pido: 30s
    policies:
    - type: Percent
      value: 100  # Puede duplicar pods inmediatamente
      periodSeconds: 60
  scaleDown:
    stabilizationWindowSeconds: 60   # Descalado rÃ¡pido: 1min (vs 5min)
    policies:
    - type: Percent
      value: 50   # Puede remover 50% de pods por minuto
      periodSeconds: 60
```

**ğŸ¯ Resultado:** 
- **Escalado**: 2 â†’ 8 pods en ~1 minuto
- **Descalado**: 8 â†’ 2 pods en ~2 minutos (vs 10+ minutos por defecto)

### **ğŸ”§ Comandos Ãštiles**

#### **Ver auto-scaling en tiempo real:**
```bash
# Terminal 1: Ver HPA cambiando
kubectl get hpa -w

# Terminal 2: Ver pods escalando  
kubectl get pods -w

# Terminal 3: Generar carga CPU
kubectl exec -it deployment/worker-deployment -- sh -c "while true; do :; done"
```

#### **MÃ©tricas y debugging:**
```bash
# Ver mÃ©tricas de nodos
kubectl top nodes

# Ver mÃ©tricas de pods
kubectl top pods

# Describir HPA (troubleshooting)
kubectl describe hpa worker-hpa

# Ver logs de un pod especÃ­fico
kubectl logs deployment/worker-deployment --tail=20
```

#### **Port forwarding para testing:**

**ğŸ”Œ Â¿Por quÃ© necesito port-forward?**
- **Sin port-forward:** API solo accesible dentro del cluster de Kubernetes
- **Con port-forward:** Puedes usar `localhost:8000` desde tu PC como siempre

**âš ï¸ IMPORTANTE: Port-forward se ejecuta en una terminal separada porque es un proceso que queda corriendo:**

```bash
# Terminal 1 (se queda "ocupada" escuchando):
kubectl port-forward service/api-service 8000:8000
# â†‘ Este comando NO termina - crea un "puente" entre tu PC y Kubernetes
# Forwarding from 127.0.0.1:8000 -> 8000
# [Se queda aquÃ­ corriendo...]

# Terminal 2 (libre para usar la API):
curl -X POST http://localhost:8000/api/process-batch/distributed/ \
  -H "Content-Type: application/json" \
  -d '{"filters": ["resize", "blur"]}'

# O ejecutar script de stress unificado:
cd k8s
python stress_test.py 5 10  # 5 minutos, 10 tareas por batch
```

**ğŸ’¡ Alternativa (una sola terminal):**
```bash
# Ejecutar en background con &
kubectl port-forward service/api-service 8000:8000 &

# Ahora puedes seguir usando la misma terminal
curl http://localhost:8000/api/process-batch/distributed/
```

### **âš ï¸ Troubleshooting ComÃºn**

#### **ğŸŒ Diferencias de Plataforma (AutomÃ¡ticamente detectadas)**

**âœ… Windows (detectado automÃ¡ticamente):**
- **Encoding**: Usa UTF-8 automÃ¡ticamente para evitar errores de emojis
- **Comandos**: Reemplaza `grep` con `findstr` y `wc -l` con `find /c`
- **curl**: Detecta PowerShell y usa `Invoke-WebRequest` cuando es necesario
- **Paths**: Maneja rutas de Windows correctamente

**âœ… Linux/Mac (detectado automÃ¡ticamente):**
- **Comandos**: Usa comandos nativos `grep`, `wc`, `curl`
- **Paths**: Maneja rutas Unix/POSIX

#### **ğŸ› Windows: Error de emojis/encoding**
```powershell
# Error: UnicodeEncodeError: 'charmap' codec can't encode character
# SoluciÃ³n: âœ… Ya arreglado automÃ¡ticamente en demo.py (UTF-8 encoding)
```

#### **ğŸ› Windows: Comandos grep no funcionan**
```powershell
# Error: 'grep' is not recognized as an internal or external command
# SoluciÃ³n: âœ… Ya arreglado automÃ¡ticamente - demo.py detecta Windows y usa comandos compatibles
```

#### **ğŸ› Windows: curl en PowerShell**
```powershell
# Error: curl da "Connection terminated unexpectedly"
# SoluciÃ³n: âœ… Ya arreglado automÃ¡ticamente - demo.py usa Invoke-WebRequest cuando es necesario

# Fallback manual si necesario:
curl.exe http://localhost:8000/api/metrics/
# O PowerShell nativo:
Invoke-WebRequest -Uri http://localhost:8000/api/metrics/ | Select-Object -ExpandProperty Content
```

#### **ğŸ› Dependencias Python faltantes**
```bash
# Error: ModuleNotFoundError: No module named 'requests'
# SoluciÃ³n: âœ… demo.py detecta automÃ¡ticamente y usa curl como fallback

# Para stress test completo (opcional):
pip install requests
```

#### **ğŸ› Docker cachÃ© problemÃ¡tico**
```bash
# Problema: Cambios en Dockerfile no se aplican
# SoluciÃ³n: Rebuild sin cachÃ©
docker build --no-cache -f docker/Dockerfile.api.final -t projects-api-final:latest .

# O usar tag Ãºnico:
docker build -f docker/Dockerfile.api.final -t projects-api-final:v2 .
kubectl set image deployment/api-deployment api=projects-api-final:v2
```

#### **Error: `ErrImageNeverPull`**
```bash
# Problema: Docker Compose crea imÃ¡genes con sufijos numÃ©ricos
docker images | grep projects
# projects-worker-1  âŒ
# projects-worker-2  âŒ

# SoluciÃ³n: Tag manual
docker tag projects-worker-1:latest projects-worker-final:latest
```

#### **Error: HPA muestra `<unknown>`**
```bash
# Problema: Metrics server no funciona
kubectl get hpa
# worker-hpa   <unknown>/70%  âŒ

# SoluciÃ³n: Configurar metrics server para Docker Desktop
kubectl patch deployment metrics-server -n kube-system --type='json' \
  -p='[{"op": "add", "path": "/spec/template/spec/containers/0/args/-", "value": "--kubelet-insecure-tls"}]'
```

#### **Error: Pods en `CrashLoopBackOff`**
```bash
# Ver logs para diagnosticar
kubectl logs deployment/worker-deployment --tail=20

# Errores comunes:
# - ModuleNotFoundError: No module named 'distributed'
# - Redis connection failed
```

#### **ğŸ› kubectl delete -f . falla**
```bash
# Error: Object 'Kind' is missing in 'docker-compose.yml'
# Problema: kubectl intenta leer docker-compose.yml como YAML de K8s

# SoluciÃ³n: Usar archivos especÃ­ficos
kubectl delete -f redis-deployment.yaml
kubectl delete -f api-deployment.yaml  
kubectl delete -f worker-deployment.yaml

# O mover docker-compose.yml temporalmente
mv docker-compose.yml docker-compose.yml.bak
kubectl delete -f .
```

### **ğŸ“ˆ ComparaciÃ³n: Docker Compose vs Kubernetes**

| Aspecto | Docker Compose | Kubernetes |
|---------|----------------|------------|
| **Auto-scaling** | âŒ Manual (`docker-compose scale`) | âœ… **AutomÃ¡tico (HPA)** |
| **MÃ©tricas** | âŒ Externas (psutil) | âœ… **Integradas (Metrics Server)** |
| **Decisiones** | âŒ Humanas | âœ… **AutomÃ¡ticas basadas en carga** |
| **Tolerancia fallos** | âŒ Manual restart | âœ… **Auto-restart de pods** |
| **ProducciÃ³n** | âŒ Solo desarrollo | âœ… **Listo para producciÃ³n** |
| **Learning curve** | âœ… FÃ¡cil | âš ï¸ **MÃ¡s complejo pero poderoso** |

### **ğŸ“ Valor Educativo**

Este proyecto demuestra la **evoluciÃ³n completa** de un sistema:

1. **Threading/Multiprocessing** â†’ Conceptos de concurrencia
2. **Docker Compose** â†’ OrchestraciÃ³n bÃ¡sica  
3. **Redis + Workers** â†’ Arquitectura distribuida
4. **Monitoring** â†’ Observabilidad en producciÃ³n
5. **Kubernetes** â†’ **Auto-scaling real y mÃ©tricas**

**Â¿El resultado?** Un sistema que:
- âœ… **Se escala automÃ¡ticamente** bajo carga
- âœ… **Optimiza recursos** cuando no hay trabajo
- âœ… **Funciona en cualquier plataforma** (Windows, Mac, Linux)
- âœ… **EstÃ¡ listo para producciÃ³n** con modificaciones mÃ­nimas

---

**ğŸ¯ Â¡Felicidades! Has construido un sistema distribuido con auto-scaling real funcionando en Kubernetes!** ğŸš€
